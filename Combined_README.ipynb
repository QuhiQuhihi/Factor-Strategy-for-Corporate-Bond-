{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Corporate bond data are more complex for several reasons: (i) whereas each company has a single stock, a company can have more than 100 bonds outstanding, (ii) the set of outstanding bonds changes over time, and (iii) some bond series are highly illiquid and the raw data have numerous erroneous and real outliers.[(2)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4586652) Addressing these data challenges in our project was one of the most challenging tasks. Database file(TRACE.db) and script (.py and .sql) for data-cleaning does not reflect full journey to clean data. There were more hidden cleaning and explatorary data analysis works in this project. \n",
    "\n",
    "The period of data used in our project range from 2013 to 2023 (10 years). Unlike other empirical research which incorporate as much data period as possble, our project include recent 10 years data since it captures unique era called negative yield and correspoding dramatic movement of bond market. Below image from Schroder Asset Management report shows that our project period captures unique begin-end of zero interest rate and volatile credit market regime.\n",
    "<div>\n",
    "<img src=\"./image_chapter1/schroder_negative_yield.png\" width=\"400\" height=\"400\">\n",
    "<img src=\"./image_chapter1/schroder_credit_spread_breakdown.png\" width=\"400\" height=\"400\">\n",
    "</div>\n",
    "\n",
    "For bond price data, our project used Trade Reporting and Compliance Engine (TRACE) database from WRDS. TRACE is a database managed by the Financial Industry Regulatory Authority (FINRA), a non-governmental organization that acts under the supervision of the Securities and Exchange Commission (SEC). TRACE was established to enhance market transparency by facilitating the mandatory reporting of over-the-counter (OTC) transactions in publicly traded U.S. corporate bonds, including debt securities issued by corporations and federal government agencies.\n",
    "\n",
    "\n",
    "## TRACE Database by SEC\n",
    "TRACE captures a comprehensive set of information regarding each bond transaction, including the bond's unique identifier (CUSIP), the transaction date and time, the price at which the bond was traded, and the transaction volume (i.e., the amount of the bond that was traded). This data provides a detailed and precise view of the secondary market activity for corporate bonds.\n",
    "\n",
    "One of the unique aspects of TRACE is its ability to provide real-time, transaction-level price information. Prior to the implementation of TRACE, the bond market was considerably less transparent, with investors often struggling to find accurate pricing data. By making this information available, TRACE significantly enhances market transparency, allowing investors to make more informed decisions.\n",
    "\n",
    "As mentioned, TRACE is managed by FINRA, which ensures compliance with its reporting requirements and oversees the dissemination of transaction data to promote fair and efficient markets. The primary users of TRACE data are institutional and retail investors, financial analysts, and researchers. These stakeholders use the data to assess current market conditions, perform historical analysis, and inform investment strategies. Our project used this database as researcher.\n",
    "\n",
    "Despite its significant contributions to market transparency, TRACE has some limitations. First, while it covers a broad range of bond transactions, it does not include all types of fixed-income securities; for example, transactions involving municipal bonds and certain agency securities are not reported through TRACE. Additionally, there can be a delay in the reporting of block trades (large transactions), which might temporarily skew the perceived market activity or pricing. Lastly, the depth of data provided, while extensive, may still lack some granular details that could be pertinent for high-level analysis, such as the identity of the bond buyer or seller.\n",
    "\n",
    "\n",
    "## Data Cleaning matters\n",
    "Although TRACE database is one of the most powerful and shows high level of integrity for empirical research, this database does not integrity of analysis. This limitation is not specific limitation for our project only. Question about replicability of The Corporate Bond Factor Zoo project [(1)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4589786) has been arose by AQR's research team [(2)](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4586652). However the author of The Corporate Bond Factor Zoo project initially claimed replication failure of cross-sectional corporate bond risk premium analysis as well [(8)](https://www.sciencedirect.com/science/article/pii/S0304405X18302095). Also, The Corporate Bond Factor Zoo project ICE datasets, while AQR project uses TRACE database. Based on this, our project claim that we cannot guarantee clear robust benchmark and replicability for this project. Rather we decided to derive implication from recent market data and build optimal strategy.\n",
    "  \n",
    "To enhance the reliability of our analysis on the corporate bond market, we apply stringent data cleaning rules to the transaction records obtained from the TRACE database. Specifically, we exclude bonds not publicly traded in the U.S. market, such as those issued via private placements, under Rule 144A, non-USD denominated bonds, and bonds issued by non-U.S. entities. We also remove structured notes, mortgage-backed or asset-backed securities, agency-backed securities, equity-linked, and convertible bonds. Additionally, we exclude bonds with floating coupon rates and those with less than one year until maturity. To ensure the robustness of our results, we replicate our analyses using the TRACE dataset curated by the Wharton Research Data Services (WRDS) data science team.\n",
    "\n",
    "We observe that monthly prices in the TRACE database are recorded precisely at the end of each month, ensuring that monthly returns calculations consistently use month-end prices. This consistency is crucial when employing equity characteristics to compute bond factor returns. However, certain bonds may not have transactions recorded at these specific times, potentially leading to inaccuracies in empirical research. To address this, we employ interpolation techniques for missing data or for months with no trade activity, thus maintaining the continuity and accuracy of our return calculations.\n",
    "\n",
    "In our study, we also encounter issues related to the discontinuity of operations by some companies, as evidenced by breaks in their unique identification codes. It is a well-established fact that stock tickers alone cannot reliably indicate continuity due to various corporate events such as mergers and acquisitions (M&A), or bankruptcy. To navigate these challenges, we utilize a Mapping Table from WRDS, which helps in tracking changes in company identifiers‚ÄîCUSIPs, GVKEYs, and tickers‚Äîthroughout different corporate events. The CUSIP is a nine-character alphanumeric code that uniquely identifies a North American financial security. The GVKEY is a unique identifier used by the Global Vantage database for individual companies, and a ticker is a stock's unique alphabetic name used on an exchange.\n",
    "\n",
    "If tracking the historical lineage of a bond or issuer proves untenable, we opt to exclude such data from our research. This approach helps minimize the impact of data discontinuities on our analysis, ensuring the integrity and reliability of our empirical findings. This meticulous data management strategy underscores the importance of rigorous data verification in conducting robust financial research.\n",
    "\n",
    "\n",
    "## Clean Price vs Drity Price\n",
    "In the TRACE database, the prices recorded for bond transactions are presented as clean prices, which represent the value of the bonds excluding any accrued interest. This format, while standardized, does not reflect the total cost or value of a bond at the point of transaction. For comprehensive analysis, especially when calculating total returns on bonds, it is crucial to consider the dirty price, which includes accrued interest. Accrued interest is the interest that has accumulated on the bond since the last interest payment up to the point of sale, and it directly affects the buyer's actual investment cost and subsequent yield.\n",
    "\n",
    "To address this discrepancy and enable a more accurate assessment of bond returns, we convert the clean prices from the TRACE database to dirty prices as part of our data processing. This conversion involves adding the accrued interest to the clean price of each bond for the respective transaction dates. By incorporating accrued interest, we ensure that our analysis reflects the total return perspective, providing a more realistic view of the bond's performance and its impact on investment strategies. This adjustment is vital for investors and analysts who rely on precise data to make informed decisions in the bond market.\n",
    "\n",
    "## Multi Factor Coefficient fitting\n",
    "\n",
    "### Coefficient data from The Corporate bond Factor Zoo\n",
    "The Mergent Fixed Income Securities Database (FISD) is a critical resource in the field of financial research, providing extensive data on publicly offered U.S. bonds. Managed by WRDS, FISD includes detailed information on various aspects of fixed-income securities, making it an invaluable tool for studying market trends, deal structures, issuer capital structures, and other relevant topics in fixed-income debt.\n",
    "\n",
    "In our project, you noted that FISD was used as a primary data source in significant prior research, providing a comprehensive set of bond characteristics and historical data. However, due to the unavailability of FISD for your research, we opted to replicate the procedures using alternative datasets (TRACE). This approach involved rigorous data cleaning and validation to ensure the integrity and comparability of our findings with those derived from FISD data.\n",
    "\n",
    "Our methodology reflects a careful adaptation to the constraints posed by data access, highlighting the challenges and solutions in conducting empirical research in finance without access to specific databases like FISD. By utilizing other reliable sources and ensuring thorough data processing, we maintain the academic rigor and relevance of our analysis, demonstrating adaptability and resourcefulness in empirical financial research.\n",
    "\n",
    "Here‚Äôs a summary of how The corporate bond factor zoo team(Dickerson, Alexander and Julliard, Christian and Mueller, Philippe) handled their data.\n",
    "\n",
    "1. Exclusion of Non-Public U.S. Market Bonds: The researchers removed bonds that were not publicly traded in the U.S. market. This category included bonds issued through private placements, bonds issued under Rule 144A, bonds that were not traded in USD, and bonds from issuers not based in the U.S.\n",
    "2. Removal of Specific Bond Types: They excluded bonds that were classified as structured notes, mortgage-backed or asset-backed securities, agency-backed securities, equity-linked, and convertible bonds.\n",
    "3. Exclusion Based on Coupon Type: They eliminated bonds that had a floating coupon rate from their analysis\n",
    "4. Maturity Considerations: Bonds with less than one year remaining until maturity were excluded from the dataset.\n",
    "5. Replication for Robustness: For robustness checks, they replicated their main results using the TRACE data processed by the WRDS data science team, ensuring that their findings were consistent across different data sets.\n",
    "\n",
    "These steps ensured that the data used in their study were highly relevant for assessing corporate bond market trends and minimized potential biases introduced by bonds that did not meet specific market and financial criteria.\n",
    "\n",
    "\n",
    "\n",
    "### Coefficient data from AQR\n",
    "This paper pointed out replication failure of The Corporate Bond Factor Zoo. In the research paper examining corporate bond factors, AQR team undertake a meticulous and comprehensive approach to data handling, which is crucial for the robustness and credibility of their findings. This outlines the key aspects of the data utilized in their study, emphasizing their methodical approach to data cleaning, integration, and analysis.\n",
    "\n",
    "1. Data Sources and Integration\n",
    "The primary data source for the study is the Trade Reporting and Compliance Engine (TRACE), which provides extensive transaction details for U.S. corporate bonds. However, recognizing the limitations and common errors within TRACE, the researchers supplement this with data from the Mergent Fixed Income Securities Database (FISD). This integration allows for a more comprehensive dataset by combining TRACE‚Äôs transactional data with FISD‚Äôs detailed bond issuance information, thereby enriching the dataset with additional bond characteristics that are not typically available in TRACE.\n",
    "\n",
    "2. Error Identification and Cleaning Process\n",
    "Initial data cleaning involves applying standard filters to remove obvious errors and anomalies from the TRACE data. This step is critical as it eliminates straightforward inaccuracies that could potentially skew the analysis. Following this, the researchers conduct a rigorous manual review of remaining outliers. This involves a detailed inspection of extreme values to determine whether they reflect true market events or are data errors. Such a manual review is essential in a market where transactions can vary widely due to market conditions, issuer events, or recording errors.\n",
    "\n",
    "3. Handling of Outliers\n",
    "Instead of employing traditional methods such as winsorizing, which might suppress true market extremes, the researchers opt for a manual approach to handle outliers. Each outlier is individually analyzed to confirm its validity, ensuring that the final dataset reflects accurate market behaviors and not distorted by misreported data. This method is particularly effective in maintaining the integrity of the dataset, as it retains genuine economic events while discarding data errors.\n",
    "\n",
    "4. Data Integration Techniques\n",
    "The integration with the Mergent FISD is strategically significant as it validates and complements the transaction data from TRACE. The combined data not only offer a fuller picture of each bond‚Äôs characteristics but also ensure that the analyses are based on a robust and error-minimized dataset. This integration is crucial for conducting a reliable empirical analysis, as it helps to cross-verify the information and provides a failsafe against potential biases introduced by relying on a single data source.\n",
    "\n",
    "5. Scientific and Replicability Focus\n",
    "A key focus of the research is on the replicability of the findings across different datasets and cleaning methods. The study meticulously tests the robustness of corporate bond factors using the cleaned and integrated dataset, addressing a common critique in financial research regarding the reproducibility of results. This focus is instrumental in confirming the external validity of the research findings and contributes significantly to the literature by identifying which bond factors are genuinely predictive and which are not.\n",
    "\n",
    "Overall, the data used in this research are handled with an exceptional level of diligence and scientific rigor, setting a high standard for empirical research in corporate bond markets. The approach ensures that the findings are not only robust but also replicable, providing valuable insights into the factors that drive corporate bond returns.\n",
    "\n",
    "Our project tried to replicate The Corporate Bond Factor Zoo's data cleaning process, considering AQR team's suggestion. Below is resulting regression coeffient table based on above approaches.\n",
    "\n",
    "\n",
    "### Resulting Coefficients tables\n",
    "1. Bond Market and Term Structure Factors: Includes the bond market factor and a term structure factor. (table name : coefficient_bond_mkt_term)\n",
    "\n",
    "2. Bond Factors: Contains factors specific to different bond categories. (table name : coefficient_bond_factors)\n",
    "\n",
    "3. Firm-Level Bond Factors: Provides factors formed on synthetic firm-level bonds. (table name : coefficient_bond_firm_factors)\n",
    "\n",
    "4. Equity Signal-Based Bond Factors: Features factors derived from equity signals. (table name : coefficient_equity_signals_all)\n",
    "\n",
    "5. Clustered Equity Signal-Based Bond Factors:  Includes factors based on clustered equity signals.\n",
    "equity_signals_cluster.csv\n",
    "\n",
    "6. The Corporate Bond Factor Zoo coefficient : This is updated coefficient table from zoo project (table name : coefficient_zoo_2024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Factors for our model\n",
    "Using data from previous chapter, we continoued our research about which factor is significant in explainaing corporate bond's return recently. Market factors, Firm level factors, Security level factors are included in multi factor model. Explanation about each factors are elaborated in appendix.\n",
    "\n",
    "AQR's paper examine Investment Grade (Moodys AAA to A-), Investment Grade Minus (Moodys BBB+ to BBB-), and Junk Grade (Moodys BB+ and below). However our project mainly focuss on Investment Grade and Investment Grade Minus (Moodys AAA to BBB-), which are commonly classified as investment grade. \n",
    "\n",
    "## Regression based factors model\n",
    "To isolate the unique returns attributed to specific factors independent of overall market movements, the study employs a regression model where factor returns are regressed against a composite of the corporate bond market and Treasury bond returns. The model is formalized as follows:\n",
    "\n",
    "#### $f_{t,i} = \\alpha_i + \\beta_{CMKT} * CMKT_{t} + \\beta_{TERM} * TERM_{t} + \\epsilon_t$\n",
    "\n",
    "#### $f_{t,i} = \\alpha_i + \\beta_{CMKT} * CMKT_{t} + \\beta_{TERM} * TERM_{t} + \\beta_{factor i} * FACTOR_i + \\epsilon_t$\n",
    "\n",
    "\n",
    "## Building long-short factor construction\n",
    "And we are going to examine each bond with factor $i$. So, compute $r_{t, i}$  for each signal $i$ at time $t$. And we assume that there are High, Mid, Low segments. This allow us to cover 67% of our universe, which incorporate broader and more general trends (besides outlier in dirty datasets) values into our model than Factor Zoo project. Equation for calculating each factor $i$ follows : \n",
    "#### $r_{t,i} = r_{t,high-i} - r_{t,low-i}$   \n",
    "This means that we buy high factor (Top 33%) and sell short factor (low 33%). \n",
    "\n",
    "## Multiple Testing Corrections\n",
    "Given the multiple hypotheses tested in identifying significant factors, a multiple testing correction is applied using the Benjamini-Hochberg procedure. This adjusts the false discovery rate, particularly important in the context of multiple comparisons. The adjusted significance level for a test statistic $t_i$ is determined based on its ranking r among all tested hypotheses ùëÅ:\n",
    "#### Adjusted $p_i$ = $min(1,p_i * (N/r))$\n",
    "where $p_i$ is the p-value associated with $t_i$.   \n",
    "\n",
    "## What is Benchmark Model\n",
    "Like Famma-French Model or CAPM model, we can use very simple market data driven model to examine the power of mullti-facator model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Long-only multi factor porfolio model\n",
    "This chapter summarizes the key findings from the academic paper titled \"Long-Only Style Investing: Don‚Äôt Just Mix, Integrate,\" which discusses the benefits of integrating multiple investment styles in long-only portfolios as opposed to merely mixing them. This approach aims to enhance portfolio performance by leveraging the interactions between different investment styles. Basic idea for multi factor portfolio is integrating, not mixing. [(4)](https://www.aqr.com/Insights/Research/White-Papers/Long-Only-Style-Investing) \n",
    "\n",
    "## Integrated vs Mix factors\n",
    "\n",
    "### Portfolio Mix\n",
    "Combines separate long-only portfolios for each style (e.g., value, momentum) without integration:\n",
    "```math\n",
    "Portfolio_{mix} = w_{mom} \\cdot Portfolio(ER_{mom}, TE_{target}) + (1 - w_{mom}) \\cdot Portfolio(ER_{val}, TE_{target})\n",
    "```\n",
    "Where:   \n",
    "$w_{mom}$: Weight assigned to the momentum portfolio.   \n",
    "$ER_{mom}$ and $ER_{val}$: Expected returns based on momentum and value.   \n",
    "$TE_{target}$: Target tracking error.    \n",
    "\n",
    "### Portfolio Integrate\n",
    "Aggregates information across all styles to form a combined expected return, constructing the portfolio in a single step:\n",
    "```math\n",
    "ER_{integrated} = w_{mom} \\cdot ER_{mom} + (1 - w_{mom}) \\cdot ER_{val}    \n",
    "```\n",
    "```math\n",
    "Portfolio_{integrated} = Portfolio(ER_{integrated}, TE_{target})\n",
    "```\n",
    "\n",
    "### Graphic Visualization of integrate vs mix\n",
    "<div>\n",
    "<img src=\"./image_chapter3/integrate1.png\" width=\"800\" height=\"800\">\n",
    "</div>\n",
    "\n",
    "\n",
    "## Practical Implications and Performance Analysis\n",
    "Empirical results indicate that the integrated portfolio approach outperforms the portfolio mix, providing higher excess returns and better risk-adjusted returns (information ratio):\n",
    "\n",
    "```math\n",
    "IR = \\frac{Average Excess Return}{Standard Deviation of Excess Return}\n",
    "```\n",
    "Integration reduces turnover and associated costs, enhancing trade efficiency across styles.\n",
    "\n",
    "Integrating investment styles in long-only portfolios offers substantial benefits, including enhanced performance and efficient capital use. This strategy is especially advantageous for managers seeking to maximize exposure to multiple styles without excessive risk or costs. Benefit of portfolio integration follows:\n",
    "- Integrates complementary characteristics of different styles, improving returns.\n",
    "- Reduces turnover by effectively netting trades across styles.\n",
    "- Enhances the ability to manage diversification and risk more effectively\n",
    "- Empirical evidence proves overperformance compare to portfolio mix\n",
    "\n",
    "<div>\n",
    "<img src=\"./image_chapter3/integrate2.png\" width=\"800\" height=\"400\">\n",
    "</div>\n",
    "\n",
    "<div>\n",
    "<img src=\"./image_chapter3/integrate3.png\" width=\"400\" height=\"400\">\n",
    "<img src=\"./image_chapter3/integrate4.png\" width=\"400\" height=\"400\">\n",
    "</div>\n",
    "\n",
    "## Benchmark1 - Mean-Variance Optimized Portfolio\n",
    "\n",
    "Mean-Variance Optimization (MVO) is a foundational concept in portfolio management. This approach aims to construct portfolios that maximize expected return for a given level of risk or minimize risk for a given level of expected return.\n",
    "\n",
    "The objective function for a mean-variance optimization problem can be expressed as:\n",
    "\n",
    "```math\n",
    "objective function:   \n",
    "\\min_{w} w^T \\Sigma w - \\lambda w^T \\mu\n",
    "```\n",
    "with constraint:  \n",
    "$w^T * 1 = 1$ (sum of weights equals one)   \n",
    "$w$ Non-negativity (no short selling included)   \n",
    "\n",
    "where,\n",
    "w is the vector of portfolio weights   \n",
    "$\\sigma$ is the covariance matrix of the asset returns   \n",
    "$\\mu$ is the vector of expected asset returns   \n",
    "$\\lambda$ is the risk tolerance factor, balancing between risk and return   \n",
    "\n",
    "\n",
    "## Benchmark2 - Risk Parity\n",
    "In risk parity, the portfolio is constructed to equalize the risk contribution of each asset, with the aim of achieving diversification and a more stable performance across different market environments. The risk contribution of each asset is proportionate to its weight and its marginal contribution to portfolio risk. The risk parity problem can be expressed as:\n",
    "```math\n",
    "\\min_{w} \\left( \\sum_{i=1}^{n} w_i \\frac{\\partial \\sigma_p}{\\partial w_i} - \\frac{\\sigma_p}{n} \\right)^2   \n",
    "```\n",
    "\n",
    "with constraint:   \n",
    "$ w^T * 1 = 1 $ (sum of weights equals one)    \n",
    "$ w > 0 $ Non-negativity (no short selling included)   \n",
    "\n",
    "where,\n",
    "w is the vector of portfolio weights    \n",
    "$\\sigma_p$ is the portfolio standard deviation     \n",
    "$\\frac{\\partial \\sigma_p}{\\partial w_i}$ is the marginal contribution of the ith asset to portfolio risk.   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Factor Model\n",
    "\n",
    "Based on metric we built from factor model, we will check return of quantile portfolio.\n",
    "\n",
    "1st quantile return - 5th quantile return\n",
    "\n",
    "this is fundamental idea of equity long-short strategy.\n",
    "\n",
    "This can show whether po\n",
    "\n",
    "Evaluating Portfolio Strategy\n",
    "Our combined factor portfolio needs to prove it's optimality\n",
    "\n",
    "public market equivalent portfolio\n",
    "\n",
    "1/N portfolio of TRACE\n",
    "\n",
    "will be used as benchmar\n",
    "\n",
    "not only simple investment return, risk-return profile will be considered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "To be updated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How To Apply This Framework\n",
    "\n",
    "Multi factor model can be baseline for systematic credit trading\n",
    "\n",
    "Also, this can be used to valuate iliquid credit strategy."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
